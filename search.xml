<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>CS231n笔记</title>
      <link href="2020/11/12/cs231n-note/"/>
      <url>2020/11/12/cs231n-note/</url>
      
        <content type="html"><![CDATA[<h1 id="CS231n-笔记"><a href="#CS231n-笔记" class="headerlink" title="CS231n 笔记"></a>CS231n 笔记</h1><h2 id="一-nbsp-nbsp-图像分类：KNN与线性分类器"><a href="#一-nbsp-nbsp-图像分类：KNN与线性分类器" class="headerlink" title="一&nbsp;&nbsp;图像分类：KNN与线性分类器"></a>一&nbsp;&nbsp;图像分类：KNN与线性分类器</h2><h3 id="1-nbsp-nbsp-KNN算法基本原理"><a href="#1-nbsp-nbsp-KNN算法基本原理" class="headerlink" title="1&nbsp;&nbsp;KNN算法基本原理"></a>1&nbsp;&nbsp;KNN算法基本原理</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>KNN</strong>（K Nearest Neighbors）是当预测一个新的值x时， 根据它距离最近的k个点是什么类别来判断x属于哪个类别。——<label style="color:green">物以类聚，“值”以群分</label><br><img src="/2020/11/12/cs231n-note/1.1.png"><br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>注意：K值的选取和点距离的计算</strong></p><h4 id="1-1-nbsp-nbsp-距离的计算"><a href="#1-1-nbsp-nbsp-距离的计算" class="headerlink" title="1.1&nbsp;&nbsp;距离的计算"></a>1.1&nbsp;&nbsp;距离的计算</h4><p><img src="/2020/11/12/cs231n-note/1.2.png"><br><img src="/2020/11/12/cs231n-note/1.3.png"><br><a href="http://vision.stanford.edu/teaching/cs231n-demos/knn/">Dome</a></p><h4 id="1-2-nbsp-nbsp-值的选取"><a href="#1-2-nbsp-nbsp-值的选取" class="headerlink" title="1.2&nbsp;&nbsp;值的选取"></a>1.2&nbsp;&nbsp;值的选取</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;通过交叉验证（将样本数据按照一定比例，拆分出训练用的数据和验证用的数据，比如6：4拆分出部分训练数据和验证数据），从选取一个较小的K值开始，不断增加K的值，然后计算验证集合的方差，最终找到一个比较合适的K值。——找到一个临界值。  </p><center><label style="color:red">Must try them all out and see what works best.</label></center><h4 id="1-3-nbsp-nbsp-KNN的特点"><a href="#1-3-nbsp-nbsp-KNN的特点" class="headerlink" title="1.3&nbsp;&nbsp;KNN的特点"></a>1.3&nbsp;&nbsp;KNN的特点</h4><p>&nbsp;&nbsp;&nbsp;&nbsp; KNN是一种<label style="color:green">非参的，惰性的</label>算法模型。<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>非参</strong>意味着这个模型不会对数据做出任何的假设，与之相对的是线性回归（我们总会假设线性回归是一条直线）。也就是说KNN建立的模型结构是根据数据来决定的，这也比较符合现实的情况，毕竟在现实中的情况往往与理论上的假设是不相符的。  </p><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>惰性</strong>又是什么意思呢？想想看，同样是分类算法，逻辑回归需要先对数据进行大量训练（tranning），最后才会得到一个算法模型。而KNN算法却不需要，它没有明确的训练数据的过程，或者说这个过程很快。<br><strong>KNN算法优点</strong>    </p><ol><li>简单易用，相比其他算法，KNN算是比较简洁明了的算法。即使没有很高的数学基础也能搞清楚它的原理。  </li><li>模型训练时间快，上面说到KNN算法是惰性的，这里也就不再过多讲述。  </li><li>预测效果好。  </li><li>对异常值不敏感。  </li></ol><p><strong>KNN算法缺点</strong>   </p><ol><li>对内存要求较高，因为该算法存储了所有训练数据</li><li>预测阶段可能很慢</li><li>对不相关的功能和数据规模敏感  </li></ol><p><strong>总结</strong><br>&nbsp;&nbsp;&nbsp;&nbsp; KNN是将不同图片中每个像素进行比较，差值最小即相似；超参数K意味着分类时取决附近K个值的分类。</p><h2 id="二、线性分类、损失函数与最优化"><a href="#二、线性分类、损失函数与最优化" class="headerlink" title="二、线性分类、损失函数与最优化"></a>二、线性分类、损失函数与最优化</h2><h3 id="2-1-线性分类"><a href="#2-1-线性分类" class="headerlink" title="2.1 线性分类"></a>2.1 线性分类</h3><p><img src="/2020/11/12/cs231n-note/2.2.png"><br>&nbsp;&nbsp;&nbsp;&nbsp;（1）线性分类由单层感知机可分类；非线性分类需要多层感知机。<br>&nbsp;&nbsp;&nbsp;&nbsp;如<strong>图一：</strong>线性映射（从图像到标签分值的参数化映射）：$f(x_i,W,b)=Wx_i+b$<br>&nbsp;&nbsp;&nbsp;&nbsp;PS：参数<strong>W</strong>被称为权重（weights），<strong>b</strong>被称为偏差向量（bias vector），矩阵<strong>W</strong>和列向量<strong>b</strong>为该函数的参数（parameters）。<br> &nbsp;&nbsp;&nbsp;&nbsp;（2）如<strong>图三</strong>有三个<strong>分类器</strong>，其中每个图像是一个点，有3个分类器。以红色的汽车分类器为例，红线表示空间中汽车分类分数为0的点的集合，红色的箭头表示分值上升的方向。所有红线右边的点的分数值均为正，且线性升高。红线左边的点分值为负，且线性降低。<br> &nbsp;&nbsp;&nbsp;&nbsp;（3）偏差和权重的合并技巧<img src="/2020/11/12/cs231n-note/2.3.jpg"></p><h3 id="2-2-损失函数-Loss-function"><a href="#2-2-损失函数-Loss-function" class="headerlink" title="2.2 损失函数 Loss function"></a>2.2 损失函数 Loss function</h3><p> &nbsp;&nbsp;&nbsp;&nbsp;神经网络以某个指标（<strong>损失函数</strong>）为线索寻找最优权重参数。我们将使用损失函数（Loss Function）来衡量我们对结果的不满意程度，直观地讲，当评分函数输出结果与真实结果之间差异越大，损失函数输出越大，反之越小。  </p><h4 id="2-2-1-均方误差"><a href="#2-2-1-均方误差" class="headerlink" title="2.2.1 均方误差"></a>2.2.1 均方误差</h4><p>$$<br>E=1 \over 2\displaystyle \sum_k{(y_k-t_k)^2}<br>$$</p><p>&nbsp;&nbsp;&nbsp;&nbsp;PS:$y_k$表示神经网络的输出，$t_k$表示监督数据，$k$表示数据的维数。</p><h4 id="2-2-2-交叉熵误差"><a href="#2-2-2-交叉熵误差" class="headerlink" title="2.2.2 交叉熵误差"></a>2.2.2 交叉熵误差</h4><p>$$<br>E=​-\displaystyle \sum_k{y_klogy_k}<br>$$</p><p> &nbsp;&nbsp;&nbsp;&nbsp;PS:$log$表示($log_e$)，$y_k$表示神经网络的输出，$t_k$表示正确解标签。</p><h4 id="2-2-3-多类支持向量机损失-Multiclass-Support-Vector-Machine-Loss"><a href="#2-2-3-多类支持向量机损失-Multiclass-Support-Vector-Machine-Loss" class="headerlink" title="2.2.3 多类支持向量机损失 Multiclass Support Vector Machine Loss"></a>2.2.3 多类支持向量机损失 Multiclass Support Vector Machine Loss</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>SVM</strong>分类器想要SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界值 Δ  ( Δ 值一般取1，代表间隔)。<br>&nbsp;&nbsp;&nbsp;&nbsp;SVM的损失函数：第i个数据中包含图像$x_i$的像素和代表正确类别的标签$y_i$。针对第j个类别的得分就是第j个元素：$s_j=f(x_i,W)<em>j$，针对第i个数据的多类SVM的损失函数定义如下：<br>$$<br>L_i=\displaystyle\sum_{j\neq y_i}{max(1,s_j-s</em>{y_i}+1)}<br>$$</p><h4 id="2-2-4-Softmax分类器"><a href="#2-2-4-Softmax分类器" class="headerlink" title="2.2.4 Softmax分类器"></a>2.2.4 Softmax分类器</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;Softmax分类器就可以理解为逻辑回归分类器面对多个分类的一般化归纳。SVM将输出[公式]作为每个分类的评分（因为无定标，所以难以直接解释）。<br>&nbsp;&nbsp;&nbsp;&nbsp;（1）Softmax函数：<br>$$<br>y_k={exp(a_k)} \over {\sum^n_{i=1}exp(a_i)}<br>$$<br> &nbsp;&nbsp;&nbsp;&nbsp;PS:假设输出层共有n个神经元，计算第k个神经元的输出$y_k$。分子是输入信号$a_k$的指数函数，分母是所有输入信号的指数函数的和。<br> <img src="/2020/11/12/cs231n-note/2.4.jpg"><br> &nbsp;&nbsp;&nbsp;&nbsp;Softmax函数的缺陷:<strong>溢出问题</strong>。（$e^1000$的结果返回inf，在超大值之间进行除法运算，结果会出现“不确定”的情况。）<br> Softmax函数的改进：<br>$$<br>y_k={exp(a_k+C^{‘})} \over {\sum^n_{i=1}exp(a_i+C^{‘})}<br>$$<br>  PS:$C^{‘}$可以为任意值，但为了防止溢出，一般使用输入信号中的最大值。<br>&nbsp;&nbsp;&nbsp;&nbsp;（2）Softmax的损失函数：<br>$$<br>L=-log({e^s{y_i}\over {\sum_j{e^{s_j}}}})<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;（3）Softmax函数的特征：<br>&nbsp;&nbsp;&nbsp;&nbsp;①输出是0.0到1.0之间（概率），输出总和为1；<br>&nbsp;&nbsp;&nbsp;&nbsp;②神经网络只把输出最大的神经元所对应的类别作为识别结果，并且位置不变。<br> &nbsp;&nbsp;&nbsp;&nbsp;（4）SVM和Softmax的比较<br> <img src="/2020/11/12/cs231n-note/2.5.jpg">  </p><h3 id="2-3-最优化Optimization"><a href="#2-3-最优化Optimization" class="headerlink" title="2.3 最优化Optimization"></a>2.3 最优化Optimization</h3><h4 id="2-3-1-参数更新"><a href="#2-3-1-参数更新" class="headerlink" title="2.3.1 参数更新"></a>2.3.1 参数更新</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;神经网络的学习<strong>目的</strong>是找到使损失函数的值尽可能小的参数，这是寻找最优参数的问题，解决这个问题的过程称为==最优化==。<br><strong>策略#1：随机搜索</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;随机尝试很多不同的权重，然后看其中哪个最好。<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>核心思路</strong>：迭代优化。</p><blockquote><p>我们的策略是从随机权重开始，然后迭代取优，从而获得更低的损失值。  </p></blockquote><p><strong>策略#2：随机本地搜索</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;第一个策略可以看做是每走一步都尝试几个随机方向，如果某个方向是向山下的，就向该方向走一步。这次我们从一个随机$W$开始，然后生成一个随机的扰动$\partial W$，只有当$W+\partial W$的损失值变低，我们才会更新。<br><strong>策略#3：随机梯度下降法（SGD）</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;使用参数的梯度，沿梯度方向更新参数，并重复这个步骤多次，从而逐渐靠近参数。<br>&nbsp;&nbsp;&nbsp;&nbsp;SGD策略：朝着当前所在位置的坡度最大的方向前进。<br>&nbsp;&nbsp;&nbsp;&nbsp;公式：$W{\leftarrow}W-\eta{\frac{\partial L}{\partial W}}$<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>SGD的缺点：</strong>如果函数的形状非均向，搜素的路径就会非常低效。SGD低效的根本原因是，梯度的方向并没有指向最小值的方向。<br><img src="/2020/11/12/cs231n-note/2.6.jpeg"><br><strong>策略#4：Momentum</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;公式：<br>$$<br>v{\leftarrow}\alpha v-\eta{\frac{\partial L}{\partial W}}<br>$$</p><p>$$<br>W{\leftarrow}W+v<br>$$</p><p>&nbsp;&nbsp;&nbsp;&nbsp;Momentum更新路径就像小球在碗中滚动一样。和SGD相比，可以更快地朝$x$轴方向靠近，<strong>减弱“之”字形的变动程度</strong>。<br><img src="/2020/11/12/cs231n-note/2.7.jpeg"><br><strong>策略#5：AdaGrad</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>学习率衰减：</strong>随着学习的进行，是学习率追渐减小。<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>AdaGrad：</strong>为参数的每个元素适当调整学习率，与此同时进行学习。<br>&nbsp;&nbsp;&nbsp;&nbsp;公式：<br>$$<br>h{\leftarrow}h+{\frac{\partial L}{\partial W}} \cdot {\frac{\partial L}{\partial W}}<br>$$</p><p>$$<br>W{\leftarrow}W-\eta{1 \over \sqrt h}{\frac{\partial L}{\partial W}}<br>$$</p><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>如图</strong>，函数的取值高效地向着最小值移动。由于$y$轴方向上的梯度较大，因此刚开始变动较大，但是后面会根据这个较大的变动按比例进行调整，减小更新的步伐。因此，$y$轴方向上的更新程度被减弱，<strong>“之”字形的变动程度有所衰减。</strong><br><img src="/2020/11/12/cs231n-note/2.8.jpeg"><br><strong>策略#6：Adam</strong>     </p><p>​</p><center><label style="color:red">Adam=Momentum+AdaGrad**</label></center><p></p><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>Momentum</strong>参照小球在碗中滚动的物理规则进行移动，<strong>AdaGrad</strong>为参数的每个元素适当地调整更新步伐。两者融合就是<strong>Adam</strong>方法的基本思路。<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>Adam的特征</strong>：进行超参数的“<label style="color:green">偏置校正</label>”。<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>如图</strong>，相比Momentum，Adam的小球左右摇晃的程度有所减轻。</p><p><img src="/2020/11/12/cs231n-note/2.9.jpeg"></p><h2 id="三、神经网络与反向传播"><a href="#三、神经网络与反向传播" class="headerlink" title="三、神经网络与反向传播"></a>三、神经网络与反向传播</h2><h3 id="3-1-神经网络-Neural-Network"><a href="#3-1-神经网络-Neural-Network" class="headerlink" title="3.1 神经网络 Neural Network"></a>3.1 神经网络 Neural Network</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>如图</strong>，神经网络的例子：<br><img src="/2020/11/12/cs231n-note/3.1.jpeg"><br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>如图</strong>，显示<strong>激活函数</strong>的计算过程：<br><img src="/2020/11/12/cs231n-note/3.2.jpeg"> </p><h4 id="3-1-1-激活函数-Activation-Function"><a href="#3-1-1-激活函数-Activation-Function" class="headerlink" title="3.1.1 激活函数 Activation Function"></a>3.1.1 激活函数 Activation Function</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>激活函数</strong>以<strong>阈值</strong>为界，一旦输入超过阈值，就切换输出。</p><h5 id="3-1-1-1-sigmoid函数"><a href="#3-1-1-1-sigmoid函数" class="headerlink" title="3.1.1.1 sigmoid函数"></a>3.1.1.1 sigmoid函数</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;公式：<br>$$<br>h(x)={1 \over {1+exp(-x)}}<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>如图</strong>，sigmoid函数具有平滑性，可以返回0.0～1.0的实数（神经网络中流动的是连续的实数值信号）。<br><img src="/2020/11/12/cs231n-note/3.3.jpg"> </p><h5 id="3-1-1-2-ReLU函数"><a href="#3-1-1-2-ReLU函数" class="headerlink" title="3.1.1.2 ReLU函数"></a>3.1.1.2 ReLU函数</h5><p>$$<br>f(x) = \begin{cases}<br>0 &amp; x &lt; 0 \<br>x &amp;  x \geq  1<br>\end{cases}<br>$$</p><p><img src="/2020/11/12/cs231n-note/3.4.jpeg"></p><h5 id="3-1-1-3-汇总"><a href="#3-1-1-3-汇总" class="headerlink" title="3.1.1.3 汇总"></a>3.1.1.3 汇总</h5><p><img src="/2020/11/12/cs231n-note/3.5.jpeg"></p><h4 id="3-1-2-前向传播"><a href="#3-1-2-前向传播" class="headerlink" title="3.1.2 前向传播"></a>3.1.2 前向传播</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>从输入层到第一层</strong>中激活函数的计算过程，隐藏层的加权和（加权信号和偏置的总和）用$a$表示，被激活函数转换后的信号用$z$表示，$h()$表示激活函数（sigmoid函数）。<br><img src="/2020/11/12/cs231n-note/3.6.jpeg"><br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>从第一层到第二层</strong>的信号传递<br><img src="/2020/11/12/cs231n-note/3.7.jpeg"><br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>从第二层到输出层</strong>的信号传递，输出层的激活函数与隐藏层的不同。一般地，1⃣️回归问题可以使用恒等函数；2⃣️二元分类问题可以使用sigmoid函数；3⃣️多元分类问题可以使用sofetmax函数。<br><img src="/2020/11/12/cs231n-note/3.8.jpeg"></p><h4 id="3-1-3-误差反向传播"><a href="#3-1-3-误差反向传播" class="headerlink" title="3.1.3 误差反向传播"></a>3.1.3 误差反向传播</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>全连接层</strong>的反向传播。<br>&nbsp;&nbsp;&nbsp;&nbsp;加法节点的反向传播将输入信号输出到下一个节点；乘法的反向传播会讲上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游。<br><img src="/2020/11/12/cs231n-note/3.9.jpeg"><br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>输出层Softmax层</strong>的反向传播<br><img src="/2020/11/12/cs231n-note/3.10.jpeg"></p><h4 id="3-1-4-神经网络实现全貌"><a href="#3-1-4-神经网络实现全貌" class="headerlink" title="3.1.4 神经网络实现全貌"></a>3.1.4 神经网络实现全貌</h4><p><img src="/2020/11/12/cs231n-note/3.11.jpeg"></p><h2 id="四、卷积神经网络"><a href="#四、卷积神经网络" class="headerlink" title="四、卷积神经网络"></a>四、卷积神经网络</h2><h3 id="4-1-CNN整体结构"><a href="#4-1-CNN整体结构" class="headerlink" title="4.1 CNN整体结构"></a>4.1 CNN整体结构</h3><p><img src="/2020/11/12/cs231n-note/4.1.jpeg">  </p><h3 id="4-2-卷积层"><a href="#4-2-卷积层" class="headerlink" title="4.2 卷积层"></a>4.2 卷积层</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;全连接层存在数据的形状被“忽视”的问题，而卷积层可以保持形状不变。</p><h4 id="4-2-1-卷积计算过程"><a href="#4-2-1-卷积计算过程" class="headerlink" title="4.2.1 卷积计算过程"></a>4.2.1 卷积计算过程</h4><p><img src="/2020/11/12/cs231n-note/4.2.jpeg">  </p><p><img src="/2020/11/12/cs231n-note/4.3.jpeg">  </p><h4 id="4-2-2-填充"><a href="#4-2-2-填充" class="headerlink" title="4.2.2 填充"></a>4.2.2 填充</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;使用<strong>填充</strong>主要是为了调整输出的大小。因为如果每次进行卷积运算都会缩小空间，那么在某个时刻输出大小就有可能变为1，导致无法再应用卷积运算。<br><img src="/2020/11/12/cs231n-note/4.4.jpeg">  </p>#### 4.2.3 步幅&nbsp;&nbsp;&nbsp;&nbsp;应用滤波器的位置间隔称为**步幅（stride）**。  ![](/4.5.jpeg)  &nbsp;&nbsp;&nbsp;&nbsp; **小结**&nbsp;&nbsp;&nbsp;&nbsp;综上，增大_步幅_后，输出大小会_变小_；增大_填充_后，输出大小会_变大_。  &nbsp;&nbsp;&nbsp;&nbsp;设输入大小为（$H$，$W$），滤波器大小为（FH，FW），输出大小为（OH，OW），填充为P，步幅为S，则填充和步幅对输出的关系：  $$OH={{H+2P-FH} \over S}+1$$$$OW={{W+2P-FW} \over S}+1$$<h4 id="4-2-4-三维卷积"><a href="#4-2-4-三维卷积" class="headerlink" title="4.2.4 三维卷积"></a>4.2.4 三维卷积</h4><p><img src="/2020/11/12/cs231n-note/4.6.jpeg"><br>&nbsp;&nbsp;&nbsp;&nbsp;PS：通道数只能设定为和输入数据的通道数相同的值。</p><h3 id="4-3-池化层"><a href="#4-3-池化层" class="headerlink" title="4.3 池化层"></a>4.3 池化层</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;池化是缩小高、长方向上的空间的运算。下图为Max池化的处理顺序。一般来说，池化的窗口大小会和步幅设定成相同的值。<br><img src="/2020/11/12/cs231n-note/4.7.jpeg"><br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>池化层的特征：</strong>1⃣️没有要学习的参数；2⃣️通道数不发生变化；3⃣️对微小的位置变化具有鲁棒性（健壮）<br>&nbsp;&nbsp;&nbsp;&nbsp;PS：鲁棒性——在异常和危险情况下系统生存的能力。</p><h3 id="4-4-CNN的可视化"><a href="#4-4-CNN的可视化" class="headerlink" title="4.4 CNN的可视化"></a>4.4 CNN的可视化</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;如果堆叠了多层卷积层，则随着层次加深，提取的信息也愈加复杂、抽象；随着层次加深，神经元从简单的形状向“高级”信息变化。<br><img src="/2020/11/12/cs231n-note/4.8.jpeg"> </p><h2 id="五、循环神经网络"><a href="#五、循环神经网络" class="headerlink" title="五、循环神经网络"></a>五、循环神经网络</h2><p><a href="https://zybuluo.com/hanbingtao/note/541458">牛逼的网站</a><br><img src="/2020/11/12/cs231n-note/5.1.jpg"><br>循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。<br><img src="/2020/11/12/cs231n-note/5.2.jpg"><br><img src="/2020/11/12/cs231n-note/5.3.jpg"><br>&nbsp;&nbsp;&nbsp;&nbsp;LSTM<br><img src="/2020/11/12/cs231n-note/5.4.png"> </p><h2 id="六、生成模型"><a href="#六、生成模型" class="headerlink" title="六、生成模型"></a>六、生成模型</h2><h2 id="七、检测与分割"><a href="#七、检测与分割" class="headerlink" title="七、检测与分割"></a>七、检测与分割</h2><h2 id="八、可视化与理解"><a href="#八、可视化与理解" class="headerlink" title="八、可视化与理解"></a>八、可视化与理解</h2><h2 id="九、增加-AI-的公正、可靠和透明度"><a href="#九、增加-AI-的公正、可靠和透明度" class="headerlink" title="九、增加 AI 的公正、可靠和透明度"></a>九、增加 AI 的公正、可靠和透明度</h2><h2 id="十、以人为本的人工智能"><a href="#十、以人为本的人工智能" class="headerlink" title="十、以人为本的人工智能"></a>十、以人为本的人工智能</h2><h2 id="十一、3D深度学习"><a href="#十一、3D深度学习" class="headerlink" title="十一、3D深度学习"></a>十一、3D深度学习</h2><h2 id="十二、深度强化学习"><a href="#十二、深度强化学习" class="headerlink" title="十二、深度强化学习"></a>十二、深度强化学习</h2><h2 id="十三、场景图"><a href="#十三、场景图" class="headerlink" title="十三、场景图"></a>十三、场景图</h2>]]></content>
      
      
      <categories>
          
          <category> deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS231n </tag>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo+github命令操作</title>
      <link href="2020/11/08/hexo-github-ming-ling-cao-zuo/"/>
      <url>2020/11/08/hexo-github-ming-ling-cao-zuo/</url>
      
        <content type="html"><![CDATA[<h1 id="Hexo-github命令操作"><a href="#Hexo-github命令操作" class="headerlink" title="Hexo+github命令操作"></a>Hexo+github命令操作</h1><p>为免自己忘记一些命令操作，将记录一些常用的操作：</p><pre><code>hexo new "postName" #新建文章hexo new page "pageName" #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，'ctrl + c'关闭server）hexo deploy #部署到GitHubhexo help  # 查看帮助hexo version  #查看Hexo的版本</code></pre><p>缩写：</p><pre><code>hexo n == hexo newhexo g == hexo generatehexo s == hexo serverhexo d == hexo deploy</code></pre><p>组合命令：</p><pre><code>hexo s -g #生成并本地预览hexo d -g #生成并上传</code></pre>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
