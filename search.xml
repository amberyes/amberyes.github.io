<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>movie recoed</title>
      <link href="2021/07/05/movie-recoed/"/>
      <url>2021/07/05/movie-recoed/</url>
      
        <content type="html"><![CDATA[<h1 id="电影记录"><a href="#电影记录" class="headerlink" title="电影记录"></a>电影记录</h1><h2 id="1-爱乐之城-La-La-Land"><a href="#1-爱乐之城-La-La-Land" class="headerlink" title="1 爱乐之城 La La Land"></a>1 爱乐之城 La La Land</h2><p>&nbsp;&nbsp;由一首歌<em>city of stars - Emma Stone/Ryan Gosling</em>想看一部电影，俗套的恋爱故事，最后不是happy ending的略微遗憾，最喜欢俩人最后在男主的酒吧中再次见面，男主想象中的与之前不同的选择，但可惜没有如果。</p><p>&nbsp;&nbsp;电影中的每一首BGM都很好听，之前有过这种体验的还是在《歌舞青春》和《音乐之声》:)</p><p><img src="/2021/07/05/movie-recoed/image-20210705225917105.png" alt="image-20210705225917105"></p>]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>《NTIRE 2020 Challenge on NonHomogeneous Dehazing》阅读笔记</title>
      <link href="2021/06/18/ntire2020/"/>
      <url>2021/06/18/ntire2020/</url>
      
        <content type="html"><![CDATA[<h1 id="NTIRE-2020-Challenge-on-NonHomogeneous-Dehazing"><a href="#NTIRE-2020-Challenge-on-NonHomogeneous-Dehazing" class="headerlink" title="NTIRE 2020 Challenge on NonHomogeneous Dehazing"></a>NTIRE 2020 Challenge on NonHomogeneous Dehazing</h1><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><ul><li>NH-Haze: 55对不同的室外有不均质雾图(5456×3632  with  24  bit  depth)</li><li>DenseHaze，O-Haze</li></ul><h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><ul><li>PSNR</li><li>SSIM</li><li>LPIPS</li><li>PI</li></ul><h2 id="Architectures"><a href="#Architectures" class="headerlink" title="Architectures"></a>Architectures</h2><ul><li>U-Net, ResNet, DenseNet and Inception</li></ul><h2 id="Modths"><a href="#Modths" class="headerlink" title="Modths"></a>Modths</h2><img src="/2021/06/18/ntire2020/image-20210607185116308.png" style="zoom:50%;"><h3 id="Trident-Dehazing-Network-TDN-☑️"><a href="#Trident-Dehazing-Network-TDN-☑️" class="headerlink" title="Trident Dehazing Network (TDN)☑️"></a>Trident Dehazing Network (TDN)☑️</h3><p>问题：</p><ul><li>对非均质雾没有鲁棒性</li><li>浓雾区域的信息难以估计</li></ul><p>主要思想：</p><img src="/2021/06/18/ntire2020/image-20210618202751577.png" style="zoom:50%;"><p>技术路线：</p><img src="/2021/06/18/ntire2020/image-20210607182010730.png" style="zoom:80%;"><p>==HDMGN子网的本质为求T图==</p><img src="/2021/06/18/ntire2020/image-20210607182107545.png" style="zoom:100%;"><h3 id="ECNU-KT☑️"><a href="#ECNU-KT☑️" class="headerlink" title="ECNU-KT☑️"></a>ECNU-KT☑️</h3><img src="/2021/06/18/ntire2020/image-20210607182157927.png" style="zoom:100%;"><img src="/2021/06/18/ntire2020/image-20210607182223827.png" style="zoom:100%;"><h3 id="dehaze-sneaker"><a href="#dehaze-sneaker" class="headerlink" title="dehaze sneaker"></a>dehaze sneaker</h3><img src="/2021/06/18/ntire2020/image-20210607182311549.png" style="zoom:100%;"><img src="/2021/06/18/ntire2020/image-20210607182322271.png" style="zoom:100%;"><h3 id="Spider"><a href="#Spider" class="headerlink" title="Spider"></a>Spider</h3><img src="/2021/06/18/ntire2020/image-20210607182505058.png" style="zoom:100%;"><h3 id="NTU-Dehazing"><a href="#NTU-Dehazing" class="headerlink" title="NTU Dehazing"></a>NTU Dehazing</h3><img src="/2021/06/18/ntire2020/image-20210607182555725.png" style="zoom:100%;"><h3 id="VICLAB-DoNET"><a href="#VICLAB-DoNET" class="headerlink" title="VICLAB-DoNET"></a>VICLAB-DoNET</h3><img src="/2021/06/18/ntire2020/image-20210607182608095.png" style="zoom:100%;"><h3 id="iPAL-NonLocal☑️"><a href="#iPAL-NonLocal☑️" class="headerlink" title="iPAL-NonLocal☑️"></a>iPAL-NonLocal☑️</h3><img src="/2021/06/18/ntire2020/image-20210607184514886.png" style="zoom:100%;"><h3 id="Team-JJ"><a href="#Team-JJ" class="headerlink" title="Team JJ"></a>Team JJ</h3><img src="/2021/06/18/ntire2020/image-20210607184546494.png" style="zoom:100%;"><h3 id="iPAL-EDN☑️"><a href="#iPAL-EDN☑️" class="headerlink" title="iPAL-EDN☑️"></a>iPAL-EDN☑️</h3><img src="/2021/06/18/ntire2020/image-20210607184610937.png" style="zoom:100%;"><img src="/2021/06/18/ntire2020/image-20210607184609255.png" style="zoom:100%;"><h3 id="NTUEE-LINLAB"><a href="#NTUEE-LINLAB" class="headerlink" title="NTUEE LINLAB"></a>NTUEE LINLAB</h3><p>​    The method is based on a encoder-decoder generator model with a multi-scale kernel encoder in the front (size is 3, 5, and 7). It is trained with part of the densenet-161. Next, the authors used BicycleGAN to enhance the generator. The hazy image and the difference of the hazy image and ground truth are used as the input when training the cVAE-GAN Encoder.</p><h3 id="NTUST-merg"><a href="#NTUST-merg" class="headerlink" title="NTUST-merg"></a>NTUST-merg</h3><p>​    The proposed method is based on the At − DH Network as the backbone network. The authors used use DenseNet as the pretrained model in the encoder network. On the other hand, they employed the similar structure of the DenseNet with additional residual block in the decoder network. They used two decoders to estimate A (atmosphere light) and t(transmission map). Moreover, L2 loss and perceptual loss were used as loss functions. In the loss term, they both only calculate the estimated haze-free image and ground truth loss.</p><h3 id="SIAT"><a href="#SIAT" class="headerlink" title="SIAT"></a>SIAT</h3><img src="/2021/06/18/ntire2020/image-20210607184740764.png" style="zoom:100%;"><h3 id="neptuneai"><a href="#neptuneai" class="headerlink" title="neptuneai"></a>neptuneai</h3><img src="/2021/06/18/ntire2020/image-20210607184803356.png" style="zoom:100%;"><h3 id="Neuro-avengers☑️"><a href="#Neuro-avengers☑️" class="headerlink" title="Neuro-avengers☑️"></a>Neuro-avengers☑️</h3><img src="/2021/06/18/ntire2020/image-20210607184824896.png" style="zoom:100%;"><h3 id="NITREXZ"><a href="#NITREXZ" class="headerlink" title="NITREXZ"></a>NITREXZ</h3><p>he proposed model contains a standard generative adversarial network. </p><h3 id="AISAIL"><a href="#AISAIL" class="headerlink" title="AISAIL"></a>AISAIL</h3><p>DeBlurGAN</p><img src="/2021/06/18/ntire2020/image-20210607184921495.png" style="zoom:100%;"><h3 id="ICAIS-dehaze"><a href="#ICAIS-dehaze" class="headerlink" title="ICAIS dehaze"></a>ICAIS dehaze</h3><img src="/2021/06/18/ntire2020/image-20210607184940521.png" style="zoom:100%;"><h3 id="RETINA"><a href="#RETINA" class="headerlink" title="RETINA"></a>RETINA</h3><p>This approach, named spatio-temporal retinex-inspired by an averaging of stochastic samples (STRASS) , is based on the spatio-temporal envelope retinex-inspired with a stochastic sampling framework (STRESS) [32] and also from the random spray retinex (RSR) [46]. In this work,</p><p>the authors used the idea of the relation developed in [19] replacing the envelope structure of the samples used in [32] by an average of these samples. Due to the local properties of the algorithm, this modified computation in the frame- work also impacted regions of the image far from the cam- era.</p><h3 id="hazefreeworld"><a href="#hazefreeworld" class="headerlink" title="hazefreeworld"></a>hazefreeworld</h3><img src="/2021/06/18/ntire2020/image-20210607185036447.png" style="zoom:100%;">]]></content>
      
      
      <categories>
          
          <category> deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NTIRE 2020 </tag>
            
            <tag> Dehaze </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《解析深度学习》阅读笔记</title>
      <link href="2020/11/17/analyze-deep-learning/"/>
      <url>2020/11/17/analyze-deep-learning/</url>
      
        <content type="html"><![CDATA[<h1 id="解析深度学习"><a href="#解析深度学习" class="headerlink" title="解析深度学习"></a>解析深度学习</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;记录一些刚开始没理解的名词。</p><h2 id="一-nbsp-nbsp-深度相比宽度，在网络复杂性中提升更有效"><a href="#一-nbsp-nbsp-深度相比宽度，在网络复杂性中提升更有效" class="headerlink" title="一&nbsp;&nbsp;深度相比宽度，在网络复杂性中提升更有效"></a>一&nbsp;&nbsp;深度相比宽度，在网络复杂性中提升更有效</h2><h3 id="1-1-nbsp-nbsp-网络深度"><a href="#1-1-nbsp-nbsp-网络深度" class="headerlink" title="1.1&nbsp;&nbsp;网络深度"></a>1.1&nbsp;&nbsp;网络深度</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>深度，即网络的层数，加深网络深度可以提升性能。</strong></p><h4 id="1-1-1-nbsp-nbsp-更好地拟合特征"><a href="#1-1-1-nbsp-nbsp-更好地拟合特征" class="headerlink" title="1.1.1&nbsp;&nbsp;更好地拟合特征"></a>1.1.1&nbsp;&nbsp;更好地拟合特征</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;现在的深度学习网络结构的主要模块是卷积、池化、激活，这是一个标准的非线性变换模块。更深的模型，意味着更好的<strong>非线性</strong>表达能力，可以学习更加复杂的变换，从而可以拟合更加复杂的特征输入。</p><h4 id="1-1-2-nbsp-nbsp-不同深度的网络层学习到不同特征"><a href="#1-1-2-nbsp-nbsp-不同深度的网络层学习到不同特征" class="headerlink" title="1.1.2 &nbsp;&nbsp;不同深度的网络层学习到不同特征"></a>1.1.2 &nbsp;&nbsp;不同深度的网络层学习到不同特征</h4><img src="/2020/11/17/analyze-deep-learning/v9.jpg" style="zoom:50%;"><h3 id="1-2-nbsp-nbsp-网络宽度"><a href="#1-2-nbsp-nbsp-网络宽度" class="headerlink" title="1.2&nbsp;&nbsp;网络宽度"></a>1.2&nbsp;&nbsp;网络宽度</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>宽度，即通道(channel)的数量，是让每一层学习到更加丰富的特征，比如不同方向，不同频率的纹理特征。</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;下面是AlexNet模型的第一个卷积层的96个通道，尽管其中有一些形状和纹理相似的卷积核(这将成为优化宽度的关键)，还是可以看到各种各种的模式。<br><img src="/2020/11/17/analyze-deep-learning/16055770618482.jpg" style="zoom:50%;"><br>&nbsp;&nbsp;&nbsp;&nbsp;有的是彩色有的是灰色，说明有的侧重于提取纹理信息，有的侧重于提取颜色信息。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;参考：<a href="https://www.zhihu.com/question/291790340">https://www.zhihu.com/question/291790340</a></p><h3 id="1-3-nbsp-nbsp-网络宽度和深度谁更加重要？"><a href="#1-3-nbsp-nbsp-网络宽度和深度谁更加重要？" class="headerlink" title="1.3&nbsp;&nbsp;网络宽度和深度谁更加重要？"></a>1.3&nbsp;&nbsp;网络宽度和深度谁更加重要？</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;两者都很重要，但目前的研究是模型性能对深度更加敏感，而调整宽度更加有利于提升模型性能。</p><ul><li>&nbsp;深度相关计算量是O(N)，宽度则是O(N * N)，宽度更加敏感；</li><li>&nbsp;弥补深度不足需要的代价更高，而增加宽度提升性能更快；</li><li>&nbsp;增加宽度对GPU更加友好。<br>&nbsp;&nbsp;&nbsp;&nbsp;一般，<strong>优先调整网络的宽度</strong>。</li></ul><h2 id="二-nbsp-nbsp-低秩近似、奇异值分解"><a href="#二-nbsp-nbsp-低秩近似、奇异值分解" class="headerlink" title="二&nbsp;&nbsp;低秩近似、奇异值分解"></a>二&nbsp;&nbsp;低秩近似、奇异值分解</h2><h3 id="2-nbsp-nbsp-奇异值分解"><a href="#2-nbsp-nbsp-奇异值分解" class="headerlink" title="2&nbsp;&nbsp;奇异值分解"></a>2&nbsp;&nbsp;奇异值分解</h3><h4 id="2-1-nbsp-nbsp-五个定理"><a href="#2-1-nbsp-nbsp-五个定理" class="headerlink" title="2.1&nbsp;&nbsp;五个定理"></a>2.1&nbsp;&nbsp;五个定理</h4><ul><li>定理一：A是对称矩阵，则不同特征值对应的特征向量是正交的。</li><li>定理二：矩阵和它的转置具有相同的特征值。</li><li>定理三：半正定矩阵的特征值均大于等于零。</li><li>定理四：若满足,则称是单位正交矩阵。</li><li>定理五：若矩阵的秩为r，则和秩均为r。</li></ul><h4 id="2-2-nbsp-nbsp-奇异值分解（SVD）"><a href="#2-2-nbsp-nbsp-奇异值分解（SVD）" class="headerlink" title="2.2&nbsp;&nbsp;奇异值分解（SVD）"></a>2.2&nbsp;&nbsp;奇异值分解（SVD）</h4><blockquote><p>[定义一]给定一个大小为 [公式] 的实对称矩阵 [公式] ，若对于任意长度为 [公式] 的非零向量 [公式] ，有 [公式] 恒成立，则矩阵 [公式] 是一个正定矩阵。</p></blockquote><hr><blockquote><p>[定义二]给定一个大小为 [公式] 的实对称矩阵 [公式] ，若对于任意长度为 [公式] 的向量 [公式] ，有 [公式] 恒成立，则矩阵 [公式] 是一个半正定矩阵。</p></blockquote><p>&nbsp;&nbsp;&nbsp;&nbsp;设矩阵$X \in R^{mxn}$，秩为$r$，$r \leq min(m,n)$，则该矩阵可以分解为：</p><center>$X=\left[ \begin{matrix} u_1 &amp; u_2 \cdots u_r\end{matrix} \right]\left[ \begin{matrix} \lambda_1 &amp; &amp; \\ &amp;\lambda_2 &amp; \\ &amp; &amp;\ddots\\ &amp; &amp; &amp; \lambda_r\\\end{matrix}\right]\left[ \begin{matrix} v^T_1 \\ v^T_2 \\\vdots \\ v^T_r\end{matrix} \right] $</center>&nbsp;&nbsp;&nbsp;&nbsp;也可表示为：<center>$X=\sum_{r=1}^n{\lambda_iu_iv^T_i}$</center>&nbsp;&nbsp;&nbsp;&nbsp;其中：$\lambda_i$为矩阵$XX^T$（或者$X^TX$）的非零向量,$u_i$为$XX^T$的对应特征向量，$v_i$为$X^TX$的对应特征向量，$\lambda_1 \ge \lambda_2 \ge···\lambda_r$。<h4 id="2-3-nbsp-nbsp-低秩近似"><a href="#2-3-nbsp-nbsp-低秩近似" class="headerlink" title="2.3&nbsp;&nbsp;低秩近似"></a>2.3&nbsp;&nbsp;低秩近似</h4><p><img src="/2020/11/17/analyze-deep-learning/v3.jpg"><br>&nbsp;&nbsp;&nbsp;&nbsp;对于现实中比较好的图片，往往秩比较低的，也就是图像比较规整，重点的突出点是有的，但是大部分像素是相似的；假如图片的秩很高，那结果就是图像杂乱无章，或者是噪声比较高。<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>因此，在做图像处理时，可以通过降低秩来去除图片中的噪点。</strong></p><h2 id="三-nbsp-nbsp-数据驱动"><a href="#三-nbsp-nbsp-数据驱动" class="headerlink" title="三&nbsp;&nbsp;数据驱动"></a>三&nbsp;&nbsp;数据驱动</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;数据驱动是通过移动互联网或者其他的相关软件为手段采集海量的数据，将数据进行组织形成信息，之后对相关的信息进行整合和提炼，在数据的基础上经过训练和拟合形成<strong>自动化</strong>的决策模型。</p><h2 id="四-nbsp-nbsp-激活函数"><a href="#四-nbsp-nbsp-激活函数" class="headerlink" title="四&nbsp;&nbsp;激活函数"></a>四&nbsp;&nbsp;激活函数</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;参数化ReLu作为激活函数的网络要优于使用原始ReLU的网络，同时自由度较大的各通道独享参数的参数化ReLU性能更优。<br>&nbsp;&nbsp;&nbsp;&nbsp;ReLU -&gt; Leaky ReLU、参数化ReLu、随机化ReLU和ELU</p><h2 id="五-nbsp-nbsp-独热编码（one-hot）"><a href="#五-nbsp-nbsp-独热编码（one-hot）" class="headerlink" title="五&nbsp;&nbsp;独热编码（one-hot）"></a>五&nbsp;&nbsp;独热编码（one-hot）</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;One-Hot编码是<strong>分类变量</strong>作为<strong>二进制向量</strong>的表示。这首先要求将分类值映射到整数值。然后，每个整数值被表示为二进制向量，除了整数的索引之外，它都是零值，它被标记为1。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;使用one-hot编码，将<strong>离散特征的取值扩展到欧氏空间，离散特征的某个取值就对应欧式空间的某个点</strong>，让特征之间的距离计算更加合理。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;不需要使用one-hot编码的情况：特征是离散的，并且不用one-hot编码就可以很合理地计算出距离，就没必要进行one-hot编码。<br>&nbsp;&nbsp;&nbsp;&nbsp;参考：<a href="https://www.cnblogs.com/shuaishuaidefeizhu/p/11269257.html">帅帅的飞猪</a></p><h2 id="六-nbsp-nbsp-泛化性"><a href="#六-nbsp-nbsp-泛化性" class="headerlink" title="六&nbsp;&nbsp;泛化性"></a>六&nbsp;&nbsp;泛化性</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;根据有限样本得到的网络模型对其他变量域也有良好的预测能力。</p><h2 id="七-nbsp-nbsp-过拟合"><a href="#七-nbsp-nbsp-过拟合" class="headerlink" title="七&nbsp;&nbsp;过拟合"></a>七&nbsp;&nbsp;过拟合</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;过拟合是指只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态。<br>&nbsp;&nbsp;&nbsp;&nbsp;原因：①模型拥有大量参数、表现力强；②训练数集少。</p><p>&nbsp;&nbsp;抑制过拟合：<br>&nbsp;&nbsp;①增加数据量；②适当减小参数数量：正则化（权值衰减）、dropout。<br><img src="/2020/11/17/analyze-deep-learning/v4.jpg" style="zoom:50%;"></p><h2 id="八-nbsp-nbsp-正负样本"><a href="#八-nbsp-nbsp-正负样本" class="headerlink" title="八&nbsp;&nbsp;正负样本"></a>八&nbsp;&nbsp;正负样本</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;在<strong>分类</strong>问题中，<strong>正样本</strong>就是任务所要检测的目标物；<strong>负样本</strong>的选取与场景有关，不能是与你要研究的问题毫不相关的乱七八糟的场景图片，这样的负样本并没有意义。比如，如果你要进行教室中学生的人脸识别，那么负样本就是教室的窗子、墙等等。<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>检测</strong>问题中的正负样本并非人工标注的那些框框，而是程序中（网络）生成出来的框框，也就是faster rcnn中的anchor boxes以及SSD中在不同分辨率的feature map中的默认框，这些框中的一部分被选为正样本，一部分被选为负样本，另外一部分被当作背景或者不参与运算。不同的框架有不同的策略，大致都是根据IoU的值，选取个阈值范围进行判定，在训练的过程中还需要注意均衡正负样本之间的比例。</p><h3 id="8-2IoU（Intersection-over-Union）"><a href="#8-2IoU（Intersection-over-Union）" class="headerlink" title="8.2IoU（Intersection over Union）"></a>8.2IoU（Intersection over Union）</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;IoU的全称为交并比,作为目标检测算法性能mAP计算的一个非常重要的函数。如图，IoU计算的是“预测的边框”和“真实的边框”的交集和并集的比值。<br><img src="/2020/11/17/analyze-deep-learning/v5.png"><br>&nbsp;&nbsp;&nbsp;&nbsp;我们使用loU看检测是否正确需要设定一个阈值，最常用的阈值是0.5，即如果loU&gt;0.5，则认为是真实的检测(true detection)，否则认为是错误的检测(false detection)。</p><h3 id="8-2-nbsp-nbsp-MAP（Mean-Average-Precision）"><a href="#8-2-nbsp-nbsp-MAP（Mean-Average-Precision）" class="headerlink" title="8.2&nbsp;&nbsp;MAP（Mean Average Precision）"></a>8.2&nbsp;&nbsp;MAP（Mean Average Precision）</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;均值平均精度(Mean Average Precision)特别适用于我们预测目标与类的位置的算法。同时均值平均值对于评估模型定位性能、目标监测模型性能和分割模型性能都是有用的。<br>&nbsp;&nbsp;&nbsp;&nbsp;MAP=所有类别的平均精度求和除以所有类别，即数据集中所有类的平均精度的平均值。<br><img src="/2020/11/17/analyze-deep-learning/v8.png"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;参考：<a href="https://www.cnblogs.com/zongfa/p/9783972.html">https://www.cnblogs.com/zongfa/p/9783972.html</a><br>&nbsp;&nbsp;&nbsp;&nbsp;使用MAP值时我们需要满足一下条件： </p><ul><li>MAP总是在固定的数据集上计算。</li><li>它不是量化模型输出的绝对度量，但是是一个比较好的相对度量。当我们在流行的公共数据集上计算这个度量时，这个度量可以很容易的用来比较不同目标检测方法。</li><li>据训练中类的分布情况，平均精度值可能会因为某些类别(具有良好的训练数据)非常高(对于具有较少或较差数据的类别)而言非常低。所以我们需要MAP可能是适中的，但是模型可能对于某些类非常好，对于某些类非常不好。因此建议在分析模型结果的同时查看个各类的平均精度，这些值也可以作为我们是不是需要添加更多训练样本的一个依据。</li></ul>]]></content>
      
      
      <categories>
          
          <category> deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS231n笔记</title>
      <link href="2020/11/12/cs231n-note/"/>
      <url>2020/11/12/cs231n-note/</url>
      
        <content type="html"><![CDATA[<h1 id="CS231n-笔记"><a href="#CS231n-笔记" class="headerlink" title="CS231n 笔记"></a>CS231n 笔记</h1><h2 id="一-nbsp-nbsp-图像分类：KNN与线性分类器"><a href="#一-nbsp-nbsp-图像分类：KNN与线性分类器" class="headerlink" title="一&nbsp;&nbsp;图像分类：KNN与线性分类器"></a>一&nbsp;&nbsp;图像分类：KNN与线性分类器</h2><h3 id="1-nbsp-nbsp-KNN算法基本原理"><a href="#1-nbsp-nbsp-KNN算法基本原理" class="headerlink" title="1&nbsp;&nbsp;KNN算法基本原理"></a>1&nbsp;&nbsp;KNN算法基本原理</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>KNN</strong>（K Nearest Neighbors）是当预测一个新的值x时， 根据它距离最近的k个点是什么类别来判断x属于哪个类别。——<label style="color:green">物以类聚，“值”以群分</label><br><img src="/2020/11/12/cs231n-note/1.1.png"><br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>注意：K值的选取和点距离的计算</strong></p><h4 id="1-1-nbsp-nbsp-距离的计算"><a href="#1-1-nbsp-nbsp-距离的计算" class="headerlink" title="1.1&nbsp;&nbsp;距离的计算"></a>1.1&nbsp;&nbsp;距离的计算</h4><p><img src="/2020/11/12/cs231n-note/1.2.png"><br><img src="/2020/11/12/cs231n-note/1.3.png"><br><a href="http://vision.stanford.edu/teaching/cs231n-demos/knn/">Dome</a></p><h4 id="1-2-nbsp-nbsp-值的选取"><a href="#1-2-nbsp-nbsp-值的选取" class="headerlink" title="1.2&nbsp;&nbsp;值的选取"></a>1.2&nbsp;&nbsp;值的选取</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;通过交叉验证（将样本数据按照一定比例，拆分出训练用的数据和验证用的数据，比如6：4拆分出部分训练数据和验证数据），从选取一个较小的K值开始，不断增加K的值，然后计算验证集合的方差，最终找到一个比较合适的K值。——找到一个临界值。  </p><center><label style="color:red">Must try them all out and see what works best.</label></center><h4 id="1-3-nbsp-nbsp-KNN的特点"><a href="#1-3-nbsp-nbsp-KNN的特点" class="headerlink" title="1.3&nbsp;&nbsp;KNN的特点"></a>1.3&nbsp;&nbsp;KNN的特点</h4><p>&nbsp;&nbsp;&nbsp;&nbsp; KNN是一种<label style="color:green">非参的，惰性的</label>算法模型。<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>非参</strong>意味着这个模型不会对数据做出任何的假设，与之相对的是线性回归（我们总会假设线性回归是一条直线）。也就是说KNN建立的模型结构是根据数据来决定的，这也比较符合现实的情况，毕竟在现实中的情况往往与理论上的假设是不相符的。  </p><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>惰性</strong>又是什么意思呢？想想看，同样是分类算法，逻辑回归需要先对数据进行大量训练（tranning），最后才会得到一个算法模型。而KNN算法却不需要，它没有明确的训练数据的过程，或者说这个过程很快。<br><strong>KNN算法优点</strong>    </p><ol><li>简单易用，相比其他算法，KNN算是比较简洁明了的算法。即使没有很高的数学基础也能搞清楚它的原理。  </li><li>模型训练时间快，上面说到KNN算法是惰性的，这里也就不再过多讲述。  </li><li>预测效果好。  </li><li>对异常值不敏感。  </li></ol><p><strong>KNN算法缺点</strong>   </p><ol><li>对内存要求较高，因为该算法存储了所有训练数据</li><li>预测阶段可能很慢</li><li>对不相关的功能和数据规模敏感  </li></ol><p><strong>总结</strong><br>&nbsp;&nbsp;&nbsp;&nbsp; KNN是将不同图片中每个像素进行比较，差值最小即相似；超参数K意味着分类时取决附近K个值的分类。</p><h2 id="二、线性分类、损失函数与最优化"><a href="#二、线性分类、损失函数与最优化" class="headerlink" title="二、线性分类、损失函数与最优化"></a>二、线性分类、损失函数与最优化</h2><h3 id="2-1-线性分类"><a href="#2-1-线性分类" class="headerlink" title="2.1 线性分类"></a>2.1 线性分类</h3><p><img src="/2020/11/12/cs231n-note/2.2.png"><br>&nbsp;&nbsp;&nbsp;&nbsp;（1）线性分类由单层感知机可分类；非线性分类需要多层感知机。<br>&nbsp;&nbsp;&nbsp;&nbsp;如<strong>图一：</strong>线性映射（从图像到标签分值的参数化映射）：$f(x_i,W,b)=Wx_i+b$<br>&nbsp;&nbsp;&nbsp;&nbsp;PS：参数<strong>W</strong>被称为权重（weights），<strong>b</strong>被称为偏差向量（bias vector），矩阵<strong>W</strong>和列向量<strong>b</strong>为该函数的参数（parameters）。<br> &nbsp;&nbsp;&nbsp;&nbsp;（2）如<strong>图三</strong>有三个<strong>分类器</strong>，其中每个图像是一个点，有3个分类器。以红色的汽车分类器为例，红线表示空间中汽车分类分数为0的点的集合，红色的箭头表示分值上升的方向。所有红线右边的点的分数值均为正，且线性升高。红线左边的点分值为负，且线性降低。<br> &nbsp;&nbsp;&nbsp;&nbsp;（3）偏差和权重的合并技巧<img src="/2020/11/12/cs231n-note/2.3.jpg"></p><h3 id="2-2-损失函数-Loss-function"><a href="#2-2-损失函数-Loss-function" class="headerlink" title="2.2 损失函数 Loss function"></a>2.2 损失函数 Loss function</h3><p> &nbsp;&nbsp;&nbsp;&nbsp;神经网络以某个指标（<strong>损失函数</strong>）为线索寻找最优权重参数。我们将使用损失函数（Loss Function）来衡量我们对结果的不满意程度，直观地讲，当评分函数输出结果与真实结果之间差异越大，损失函数输出越大，反之越小。  </p><h4 id="2-2-1-均方误差"><a href="#2-2-1-均方误差" class="headerlink" title="2.2.1 均方误差"></a>2.2.1 均方误差</h4><p>$$<br>E=1 \over 2\displaystyle \sum_k{(y_k-t_k)^2}<br>$$</p><p>&nbsp;&nbsp;&nbsp;&nbsp;PS:$y_k$表示神经网络的输出，$t_k$表示监督数据，$k$表示数据的维数。</p><h4 id="2-2-2-交叉熵误差"><a href="#2-2-2-交叉熵误差" class="headerlink" title="2.2.2 交叉熵误差"></a>2.2.2 交叉熵误差</h4><p>$$<br>E=​-\displaystyle \sum_k{y_klogy_k}<br>$$</p><p> &nbsp;&nbsp;&nbsp;&nbsp;PS:$log$表示($log_e$)，$y_k$表示神经网络的输出，$t_k$表示正确解标签。</p><h4 id="2-2-3-多类支持向量机损失-Multiclass-Support-Vector-Machine-Loss"><a href="#2-2-3-多类支持向量机损失-Multiclass-Support-Vector-Machine-Loss" class="headerlink" title="2.2.3 多类支持向量机损失 Multiclass Support Vector Machine Loss"></a>2.2.3 多类支持向量机损失 Multiclass Support Vector Machine Loss</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>SVM</strong>分类器想要SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界值 Δ  ( Δ 值一般取1，代表间隔)。<br>&nbsp;&nbsp;&nbsp;&nbsp;SVM的损失函数：第i个数据中包含图像$x_i$的像素和代表正确类别的标签$y_i$。针对第j个类别的得分就是第j个元素：$s_j=f(x_i,W)<em>j$，针对第i个数据的多类SVM的损失函数定义如下：<br>$$<br>L_i=\displaystyle\sum_{j\neq y_i}{max(1,s_j-s</em>{y_i}+1)}<br>$$</p><h4 id="2-2-4-Softmax分类器"><a href="#2-2-4-Softmax分类器" class="headerlink" title="2.2.4 Softmax分类器"></a>2.2.4 Softmax分类器</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;Softmax分类器就可以理解为逻辑回归分类器面对多个分类的一般化归纳。SVM将输出[公式]作为每个分类的评分（因为无定标，所以难以直接解释）。<br>&nbsp;&nbsp;&nbsp;&nbsp;（1）Softmax函数：<br>$$<br>y_k={exp(a_k)} \over {\sum^n_{i=1}exp(a_i)}<br>$$<br> &nbsp;&nbsp;&nbsp;&nbsp;PS:假设输出层共有n个神经元，计算第k个神经元的输出$y_k$。分子是输入信号$a_k$的指数函数，分母是所有输入信号的指数函数的和。<br> <img src="/2020/11/12/cs231n-note/2.4.jpg"><br> &nbsp;&nbsp;&nbsp;&nbsp;Softmax函数的缺陷:<strong>溢出问题</strong>。（$e^1000$的结果返回inf，在超大值之间进行除法运算，结果会出现“不确定”的情况。）<br> Softmax函数的改进：<br>$$<br>y_k={exp(a_k+C^{‘})} \over {\sum^n_{i=1}exp(a_i+C^{‘})}<br>$$<br>  PS:$C^{‘}$可以为任意值，但为了防止溢出，一般使用输入信号中的最大值。<br>&nbsp;&nbsp;&nbsp;&nbsp;（2）Softmax的损失函数：<br>$$<br>L=-log({e^s{y_i}\over {\sum_j{e^{s_j}}}})<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;（3）Softmax函数的特征：<br>&nbsp;&nbsp;&nbsp;&nbsp;①输出是0.0到1.0之间（概率），输出总和为1；<br>&nbsp;&nbsp;&nbsp;&nbsp;②神经网络只把输出最大的神经元所对应的类别作为识别结果，并且位置不变。<br> &nbsp;&nbsp;&nbsp;&nbsp;（4）SVM和Softmax的比较<br> <img src="/2020/11/12/cs231n-note/2.5.jpg">  </p><h3 id="2-3-最优化Optimization"><a href="#2-3-最优化Optimization" class="headerlink" title="2.3 最优化Optimization"></a>2.3 最优化Optimization</h3><h4 id="2-3-1-参数更新"><a href="#2-3-1-参数更新" class="headerlink" title="2.3.1 参数更新"></a>2.3.1 参数更新</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;神经网络的学习<strong>目的</strong>是找到使损失函数的值尽可能小的参数，这是寻找最优参数的问题，解决这个问题的过程称为==最优化==。<br><strong>策略#1：随机搜索</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;随机尝试很多不同的权重，然后看其中哪个最好。<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>核心思路</strong>：迭代优化。</p><blockquote><p>我们的策略是从随机权重开始，然后迭代取优，从而获得更低的损失值。  </p></blockquote><p><strong>策略#2：随机本地搜索</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;第一个策略可以看做是每走一步都尝试几个随机方向，如果某个方向是向山下的，就向该方向走一步。这次我们从一个随机$W$开始，然后生成一个随机的扰动$\partial W$，只有当$W+\partial W$的损失值变低，我们才会更新。<br><strong>策略#3：随机梯度下降法（SGD）</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;使用参数的梯度，沿梯度方向更新参数，并重复这个步骤多次，从而逐渐靠近参数。<br>&nbsp;&nbsp;&nbsp;&nbsp;SGD策略：朝着当前所在位置的坡度最大的方向前进。<br>&nbsp;&nbsp;&nbsp;&nbsp;公式：$W{\leftarrow}W-\eta{\frac{\partial L}{\partial W}}$<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>SGD的缺点：</strong>如果函数的形状非均向，搜素的路径就会非常低效。SGD低效的根本原因是，梯度的方向并没有指向最小值的方向。<br><img src="/2020/11/12/cs231n-note/2.6.jpeg"><br><strong>策略#4：Momentum</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;公式：<br>$$<br>v{\leftarrow}\alpha v-\eta{\frac{\partial L}{\partial W}}<br>$$</p><p>$$<br>W{\leftarrow}W+v<br>$$</p><p>&nbsp;&nbsp;&nbsp;&nbsp;Momentum更新路径就像小球在碗中滚动一样。和SGD相比，可以更快地朝$x$轴方向靠近，<strong>减弱“之”字形的变动程度</strong>。<br><img src="/2020/11/12/cs231n-note/2.7.jpeg"><br><strong>策略#5：AdaGrad</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>学习率衰减：</strong>随着学习的进行，是学习率追渐减小。<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>AdaGrad：</strong>为参数的每个元素适当调整学习率，与此同时进行学习。<br>&nbsp;&nbsp;&nbsp;&nbsp;公式：<br>$$<br>h{\leftarrow}h+{\frac{\partial L}{\partial W}} \cdot {\frac{\partial L}{\partial W}}<br>$$</p><p>$$<br>W{\leftarrow}W-\eta{1 \over \sqrt h}{\frac{\partial L}{\partial W}}<br>$$</p><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>如图</strong>，函数的取值高效地向着最小值移动。由于$y$轴方向上的梯度较大，因此刚开始变动较大，但是后面会根据这个较大的变动按比例进行调整，减小更新的步伐。因此，$y$轴方向上的更新程度被减弱，<strong>“之”字形的变动程度有所衰减。</strong><br><img src="/2020/11/12/cs231n-note/2.8.jpeg"><br><strong>策略#6：Adam</strong>     </p><center><label style="color:red">Adam=Momentum+AdaGrad**</label></center><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>Momentum</strong>参照小球在碗中滚动的物理规则进行移动，<strong>AdaGrad</strong>为参数的每个元素适当地调整更新步伐。两者融合就是<strong>Adam</strong>方法的基本思路。<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>Adam的特征</strong>：进行超参数的“<label style="color:green">偏置校正</label>”。<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>如图</strong>，相比Momentum，Adam的小球左右摇晃的程度有所减轻。</p><p><img src="/2020/11/12/cs231n-note/2.9.jpeg"></p><h2 id="三、神经网络与反向传播"><a href="#三、神经网络与反向传播" class="headerlink" title="三、神经网络与反向传播"></a>三、神经网络与反向传播</h2><h3 id="3-1-神经网络-Neural-Network"><a href="#3-1-神经网络-Neural-Network" class="headerlink" title="3.1 神经网络 Neural Network"></a>3.1 神经网络 Neural Network</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>如图</strong>，神经网络的例子：<br><img src="/2020/11/12/cs231n-note/3.1.jpeg"><br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>如图</strong>，显示<strong>激活函数</strong>的计算过程：<br><img src="/2020/11/12/cs231n-note/3.2.jpeg"> </p><h4 id="3-1-1-激活函数-Activation-Function"><a href="#3-1-1-激活函数-Activation-Function" class="headerlink" title="3.1.1 激活函数 Activation Function"></a>3.1.1 激活函数 Activation Function</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>激活函数</strong>以<strong>阈值</strong>为界，一旦输入超过阈值，就切换输出。</p><h5 id="3-1-1-1-sigmoid函数"><a href="#3-1-1-1-sigmoid函数" class="headerlink" title="3.1.1.1 sigmoid函数"></a>3.1.1.1 sigmoid函数</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;公式：<br>$$<br>h(x)={1 \over {1+exp(-x)}}<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>如图</strong>，sigmoid函数具有平滑性，可以返回0.0～1.0的实数（神经网络中流动的是连续的实数值信号）。<br><img src="/2020/11/12/cs231n-note/3.3.jpg"> </p><h5 id="3-1-1-2-ReLU函数"><a href="#3-1-1-2-ReLU函数" class="headerlink" title="3.1.1.2 ReLU函数"></a>3.1.1.2 ReLU函数</h5><p>$$<br>f(x) = \begin{cases}<br>0 &amp; x &lt; 0 \<br>x &amp;  x \geq  1<br>\end{cases}<br>$$</p><p><img src="/2020/11/12/cs231n-note/3.4.jpeg"></p><h5 id="3-1-1-3-汇总"><a href="#3-1-1-3-汇总" class="headerlink" title="3.1.1.3 汇总"></a>3.1.1.3 汇总</h5><p><img src="/2020/11/12/cs231n-note/3.5.jpeg"></p><h4 id="3-1-2-前向传播"><a href="#3-1-2-前向传播" class="headerlink" title="3.1.2 前向传播"></a>3.1.2 前向传播</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>从输入层到第一层</strong>中激活函数的计算过程，隐藏层的加权和（加权信号和偏置的总和）用$a$表示，被激活函数转换后的信号用$z$表示，$h()$表示激活函数（sigmoid函数）。<br><img src="/2020/11/12/cs231n-note/3.6.jpeg"><br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>从第一层到第二层</strong>的信号传递<br><img src="/2020/11/12/cs231n-note/3.7.jpeg"><br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>从第二层到输出层</strong>的信号传递，输出层的激活函数与隐藏层的不同。一般地，1⃣️回归问题可以使用恒等函数；2⃣️二元分类问题可以使用sigmoid函数；3⃣️多元分类问题可以使用sofetmax函数。<br><img src="/2020/11/12/cs231n-note/3.8.jpeg"></p><h4 id="3-1-3-误差反向传播"><a href="#3-1-3-误差反向传播" class="headerlink" title="3.1.3 误差反向传播"></a>3.1.3 误差反向传播</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>全连接层</strong>的反向传播。<br>&nbsp;&nbsp;&nbsp;&nbsp;加法节点的反向传播将输入信号输出到下一个节点；乘法的反向传播会讲上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游。<br><img src="/2020/11/12/cs231n-note/3.9.jpeg"><br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>输出层Softmax层</strong>的反向传播<br><img src="/2020/11/12/cs231n-note/3.10.jpeg"></p><h4 id="3-1-4-神经网络实现全貌"><a href="#3-1-4-神经网络实现全貌" class="headerlink" title="3.1.4 神经网络实现全貌"></a>3.1.4 神经网络实现全貌</h4><p><img src="/2020/11/12/cs231n-note/3.11.jpeg"></p><h2 id="四、卷积神经网络"><a href="#四、卷积神经网络" class="headerlink" title="四、卷积神经网络"></a>四、卷积神经网络</h2><h3 id="4-1-CNN整体结构"><a href="#4-1-CNN整体结构" class="headerlink" title="4.1 CNN整体结构"></a>4.1 CNN整体结构</h3><p><img src="/2020/11/12/cs231n-note/4.1.jpeg">  </p><h3 id="4-2-卷积层"><a href="#4-2-卷积层" class="headerlink" title="4.2 卷积层"></a>4.2 卷积层</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;全连接层存在数据的形状被“忽视”的问题，而卷积层可以保持形状不变。</p><h4 id="4-2-1-卷积计算过程"><a href="#4-2-1-卷积计算过程" class="headerlink" title="4.2.1 卷积计算过程"></a>4.2.1 卷积计算过程</h4><p><img src="/2020/11/12/cs231n-note/4.2.jpeg">  </p><p><img src="/2020/11/12/cs231n-note/4.3.jpeg">  </p><h4 id="4-2-2-填充"><a href="#4-2-2-填充" class="headerlink" title="4.2.2 填充"></a>4.2.2 填充</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;使用<strong>填充</strong>主要是为了调整输出的大小。因为如果每次进行卷积运算都会缩小空间，那么在某个时刻输出大小就有可能变为1，导致无法再应用卷积运算。<br><img src="/2020/11/12/cs231n-note/4.4.jpeg">  </p>#### 4.2.3 步幅&nbsp;&nbsp;&nbsp;&nbsp;应用滤波器的位置间隔称为**步幅（stride）**。  ![](/4.5.jpeg)  &nbsp;&nbsp;&nbsp;&nbsp; **小结**&nbsp;&nbsp;&nbsp;&nbsp;综上，增大_步幅_后，输出大小会_变小_；增大_填充_后，输出大小会_变大_。  &nbsp;&nbsp;&nbsp;&nbsp;设输入大小为（$H$，$W$），滤波器大小为（FH，FW），输出大小为（OH，OW），填充为P，步幅为S，则填充和步幅对输出的关系：  $$OH={{H+2P-FH} \over S}+1$$$$OW={{W+2P-FW} \over S}+1$$<h4 id="4-2-4-三维卷积"><a href="#4-2-4-三维卷积" class="headerlink" title="4.2.4 三维卷积"></a>4.2.4 三维卷积</h4><p><img src="/2020/11/12/cs231n-note/4.6.jpeg"><br>&nbsp;&nbsp;&nbsp;&nbsp;PS：通道数只能设定为和输入数据的通道数相同的值。</p><h3 id="4-3-池化层"><a href="#4-3-池化层" class="headerlink" title="4.3 池化层"></a>4.3 池化层</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;池化是缩小高、长方向上的空间的运算。下图为Max池化的处理顺序。一般来说，池化的窗口大小会和步幅设定成相同的值。<br><img src="/2020/11/12/cs231n-note/4.7.jpeg"><br>&nbsp;&nbsp;&nbsp;&nbsp;<strong>池化层的特征：</strong>1⃣️没有要学习的参数；2⃣️通道数不发生变化；3⃣️对微小的位置变化具有鲁棒性（健壮）<br>&nbsp;&nbsp;&nbsp;&nbsp;PS：鲁棒性——在异常和危险情况下系统生存的能力。</p><h3 id="4-4-CNN的可视化"><a href="#4-4-CNN的可视化" class="headerlink" title="4.4 CNN的可视化"></a>4.4 CNN的可视化</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;如果堆叠了多层卷积层，则随着层次加深，提取的信息也愈加复杂、抽象；随着层次加深，神经元从简单的形状向“高级”信息变化。<br><img src="/2020/11/12/cs231n-note/4.8.jpeg"> </p><h2 id="五、循环神经网络"><a href="#五、循环神经网络" class="headerlink" title="五、循环神经网络"></a>五、循环神经网络</h2><p><a href="https://zybuluo.com/hanbingtao/note/541458">牛逼的网站</a><br><img src="/2020/11/12/cs231n-note/5.1.jpg"><br>循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。<br><img src="/2020/11/12/cs231n-note/5.2.jpg"><br><img src="/2020/11/12/cs231n-note/5.3.jpg"><br>&nbsp;&nbsp;&nbsp;&nbsp;LSTM<br><img src="/2020/11/12/cs231n-note/5.4.png"> </p><h2 id="六、生成模型"><a href="#六、生成模型" class="headerlink" title="六、生成模型"></a>六、生成模型</h2><h2 id="七、检测与分割"><a href="#七、检测与分割" class="headerlink" title="七、检测与分割"></a>七、检测与分割</h2><h2 id="八、可视化与理解"><a href="#八、可视化与理解" class="headerlink" title="八、可视化与理解"></a>八、可视化与理解</h2><h2 id="九、增加-AI-的公正、可靠和透明度"><a href="#九、增加-AI-的公正、可靠和透明度" class="headerlink" title="九、增加 AI 的公正、可靠和透明度"></a>九、增加 AI 的公正、可靠和透明度</h2><h2 id="十、以人为本的人工智能"><a href="#十、以人为本的人工智能" class="headerlink" title="十、以人为本的人工智能"></a>十、以人为本的人工智能</h2><h2 id="十一、3D深度学习"><a href="#十一、3D深度学习" class="headerlink" title="十一、3D深度学习"></a>十一、3D深度学习</h2><h2 id="十二、深度强化学习"><a href="#十二、深度强化学习" class="headerlink" title="十二、深度强化学习"></a>十二、深度强化学习</h2><h2 id="十三、场景图"><a href="#十三、场景图" class="headerlink" title="十三、场景图"></a>十三、场景图</h2>]]></content>
      
      
      <categories>
          
          <category> deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> CS231n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo+github命令操作</title>
      <link href="2020/11/08/hexo-github-ming-ling-cao-zuo/"/>
      <url>2020/11/08/hexo-github-ming-ling-cao-zuo/</url>
      
        <content type="html"><![CDATA[<h1 id="Hexo-github命令操作"><a href="#Hexo-github命令操作" class="headerlink" title="Hexo+github命令操作"></a>Hexo+github命令操作</h1><p>为免自己忘记一些命令操作，将记录一些常用的操作：</p><pre><code>hexo new "postName" #新建文章hexo new page "pageName" #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，'ctrl + c'关闭server）hexo deploy #部署到GitHubhexo help  # 查看帮助hexo version  #查看Hexo的版本</code></pre><p>缩写：</p><pre><code>hexo n == hexo newhexo g == hexo generatehexo s == hexo serverhexo d == hexo deploy</code></pre><p>组合命令：</p><pre><code>hexo s -g #生成并本地预览hexo d -g #生成并上传</code></pre>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
